# Ch33nchan-Torch ðŸš€

## Project Overview

The **end goal** of this project is to replicate PyTorch **from scratch**, using only **C++**. Currently, the repository focuses on building the **core foundations** required to handle tensors, perform mathematical operations, and implement neural networks.

### Current Features:
1. **Tensor Creation**: Implementation of multi-dimensional arrays (tensors) with strides for efficient memory representation.
2. **Matrix Multiplication**: Fundamental operations necessary for forward propagation and backpropagation.
3. **Autograd Mechanism**: Automatic differentiation to compute gradients for optimization.
4. **Loss Functions**: Implementation of Mean Squared Error (MSE).
5. **Simple Neural Networks**: Implementation of a feedforward neural network.

---

## Mathematical Foundations

This project delves into **tensor operations** and **neural network mechanics**, implementing them at the most fundamental level. Below is a deeper dive into the mathematical and technical details.

---

### Tensors

A **tensor** is a generalization of scalars, vectors, and matrices to higher dimensions. In this implementation:

- **0D Tensor**: Scalar (e.g., 3.0)
- **1D Tensor**: Vector (e.g., [1.0, 2.0, 3.0])
- **2D Tensor**: Matrix (e.g., [[1, 2], [3, 4]])
- **nD Tensor**: Multi-dimensional array

**Stride Mechanism:** Strides are used to map multidimensional tensor indices to a flat array in memory.

For example:
- A tensor of shape `(2, 3)` with strides `[3, 1]` means the value at `(i, j)` maps to `i * 3 + j` in memory.

### Code Example: Tensor Creation

```cpp
Tensor tensor({2, 3}, {1.0, 2.0, 3.0, 4.0, 5.0, 6.0}); // A 2x3 tensor
tensor.print();  // Outputs the tensor in matrix format
```

---

### Matrix Multiplication

Matrix multiplication is the **core operation** of many machine learning algorithms, including neural networks.

#### Mathematical Definition:

Given two matrices $A$ and $B$:
$$
C_{ij} = \sum_{k=1}^n A_{ik} \cdot B_{kj}
$$

- **Dimensions**: If $A$ is $m \times n$ and $B$ is $n \times p$, then $C$ will be $m \times p$.

#### Code Example: Matrix Multiplication

Matrix multiplication is implemented in `mat_mul.cpp`. Here's the simplified logic:

```cpp
for (size_t i = 0; i < A_rows; ++i) {
    for (size_t j = 0; j < B_cols; ++j) {
        float sum = 0.0;
        for (size_t k = 0; k < A_cols; ++k) {
            sum += A[i][k] * B[k][j];
        }
        C[i][j] = sum;
    }
}
```

#### Example:

Input matrices:

$$
A = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}, \quad
B = \begin{bmatrix} 5 & 6 \\ 7 & 8 \end{bmatrix}
$$

Output:

$$
C = A \cdot B = \begin{bmatrix} 19 & 22 \\ 43 & 50 \end{bmatrix}
$$

---

### Autograd

Autograd is the mechanism for computing **gradients** automatically. This is crucial for backpropagation in neural networks.

#### Gradient Computation

If $y = f(x)$, the gradient $\frac{\partial y}{\partial x}$ is computed using the **chain rule**:

$$
\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial x}
$$

#### Backward Pass

Gradients are propagated backward through the computation graph.

Example:
For $z = x \cdot y$, where $x = 2, y = 3$:
$$
\frac{\partial z}{\partial x} = y, \quad \frac{\partial z}{\partial y} = x
$$

Code Snippet:

```cpp
float grad_x = y;  // Partial derivative w.r.t x
float grad_y = x;  // Partial derivative w.r.t y
```

---

### Loss Function

The **Mean Squared Error (MSE)** loss is defined as:

$$
\text{MSE} = \frac{1}{n} \sum_{i=1}^n (y_{\text{pred}}^{(i)} - y_{\text{true}}^{(i)})^2
$$

This is implemented in `loss_func.cpp`.

Code Example:

```cpp
float mse = 0.0;
for (size_t i = 0; i < y_pred.size(); ++i) {
    mse += std::pow(y_pred[i] - y_true[i], 2);
}
mse /= y_pred.size();
```

---

### Neural Network

The neural network is implemented as a feedforward network with one hidden layer.

#### Forward Propagation

1. Multiply inputs with weights.
2. Add biases.
3. Apply an activation function (ReLU).

Mathematical Representation:
$$
y = \text{ReLU}(X \cdot W + b)
$$

#### Backpropagation

Gradients are calculated using the chain rule. The weights and biases are updated using gradient descent:

$$
W_{\text{new}} = W_{\text{old}} - \eta \cdot \frac{\partial L}{\partial W}
$$

where $\eta$ is the learning rate.

Code Example:

```cpp
// Forward Pass
Tensor hidden = relu(matmul(input, weights_hidden) + bias_hidden);
Tensor output = matmul(hidden, weights_output) + bias_output;

// Backward Pass
Tensor grad_output = output - target;
weights_output -= learning_rate * matmul(hidden.T(), grad_output);
```

---

## File Structure

```
ch33nchan-torch/
â”œâ”€â”€ autograd.cpp         # Implements automatic differentiation (gradients)
â”œâ”€â”€ autograd_main.cpp    # Entry point to test autograd and tensors
â”œâ”€â”€ loss_func.cpp        # Implements the Mean Squared Error (MSE) loss
â”œâ”€â”€ mat_mul.cpp          # Implements matrix multiplication
â”œâ”€â”€ ndarray.hpp          # Header file for tensor operations
â”œâ”€â”€ ndaray-stride.cpp    # Implements stride-based tensor indexing
â”œâ”€â”€ neural_network.cpp   # Implements a simple neural network
```

---

## How to Use

1. Clone the repository:
   ```bash
   git clone https://github.com/your-repo/ch33nchan-torch.git
   ```

2. Compile and run:
   ```bash
   g++ -std=c++17 -o neural_network neural_network.cpp -O2
   ./neural_network
   ```

3. Extend functionality by adding new operations to tensors or enhancing the autograd mechanism.

---

## Future Work

- Expand tensor operations (e.g., convolution, pooling)
- Implement more advanced loss functions
- Add support for recurrent neural networks
- Optimize tensor memory management

---

## Contributing

Feel free to fork and submit pull requests! Let's make this as powerful as PyTorch! ðŸš€



### N3xt steps : 
Based on your current working directory, hereâ€™s a suggested progression for your Torch++ project and the Chess AI project:

Next Steps for Torch++
	1.	Files to Generate:
	â€¢	optimizer.cpp: Implement optimizers like SGD and Adam.
	â€¢	dataset.cpp: Provide utilities for handling datasets (e.g., loading, batching).
	â€¢	activation_functions.cpp: Centralize all activation functions like ReLU, sigmoid, and log-softmax.
	â€¢	dropout.cpp: Introduce dropout functionality for regularization.
	â€¢	convolution.cpp: Implement 1D and 2D convolution layers.
	â€¢	pooling.cpp: Add max and average pooling layers.
	â€¢	serialization.cpp: Handle saving and loading models.
	â€¢	torchplusplus.hpp: Centralized header to include all modules for ease of use.

Overview of Chess AI Project

Goal: Build a 2-player chess game where one player competes against an AI powered by a model trained with the Torch++ library.

File List for Chess AI Project
	â€¢	main.cpp:
Entry point for initializing the game, handling the main loop, and user input.
	â€¢	chess_board.cpp:
Implements the board, including piece placement, movement, and visualization.
	â€¢	chess_board.hpp:
Header file for the chess board structure and functionality.
	â€¢	chess_ai.cpp:
Handles the AI logic, including move evaluation and decision-making.
	â€¢	chess_ai.hpp:
Header file for AI-related functions.
	â€¢	game_rules.cpp:
Implements the rules of chess (e.g., valid moves, checks, castling).
	â€¢	game_rules.hpp:
Header file for chess rules.
	â€¢	minimax.cpp:
Implements the Minimax algorithm with optional alpha-beta pruning for decision-making.
	â€¢	neural_network_agent.cpp:
Uses the Torch++ library to load a trained model for AI-based moves.
	â€¢	evaluation_function.cpp:
Defines heuristics or scoring functions for evaluating board states.
	â€¢	utils.cpp:
Helper functions for tasks like move parsing, board setup, and printing.
	â€¢	torch_model_training.cpp:
Prepares and trains the chess-playing AI model using Torch++.

High-Level Flow
	1.	Initialize Game: Load the board and set up players (human vs. AI or AI vs. AI).
	2.	Input Handling: Accept user input for moves or let the AI determine its move.
	3.	Rule Enforcement: Validate moves using game_rules.cpp.
	4.	AI Decision-Making:
	â€¢	Use minimax.cpp for heuristic-based decision-making.
	â€¢	Optionally switch to neural_network_agent.cpp for model-driven moves.
	5.	Game Visualization: Render the board and display updates (e.g., moves, captures, check).
	6.	End Conditions: Check for game-over scenarios (checkmate, stalemate).

torchplusplus/
â”œâ”€â”€ CMakeLists.txt                 # Main CMake configuration file
â”œâ”€â”€ README.md                      # Project documentation
â”œâ”€â”€ .gitignore                     # Git ignore file
â”œâ”€â”€ include/                       # Public header files
â”‚   â””â”€â”€ torchplusplus/
â”‚       â”œâ”€â”€ tensor.hpp            # Tensor class declaration
â”‚       â”œâ”€â”€ autograd.hpp          # Autograd system declarations
â”‚       â”œâ”€â”€ nn/                   # Neural network components
â”‚       â”‚   â”œâ”€â”€ linear.hpp        # Linear layer
â”‚       â”‚   â”œâ”€â”€ conv2d.hpp        # Convolution layer
â”‚       â”‚   â”œâ”€â”€ activation.hpp    # Activation functions
â”‚       â”‚   â””â”€â”€ loss.hpp          # Loss functions
â”‚       â”œâ”€â”€ ops/                  # Operations
â”‚       â”‚   â”œâ”€â”€ basic_ops.hpp     # Basic arithmetic operations
â”‚       â”‚   â”œâ”€â”€ matrix_ops.hpp    # Matrix operations
â”‚       â”‚   â””â”€â”€ reduction_ops.hpp # Reduction operations (sum, mean, etc.)
â”‚       â””â”€â”€ utils/
â”‚           â”œâ”€â”€ ndarray.hpp       # N-dimensional array utilities
â”‚           â””â”€â”€ exceptions.hpp    # Custom exceptions
â”‚
â”œâ”€â”€ src/                          # Implementation files
â”‚   â”œâ”€â”€ tensor.cpp               # Tensor class implementation
â”‚   â”œâ”€â”€ autograd.cpp             # Autograd system implementation
â”‚   â”œâ”€â”€ nn/
â”‚   â”‚   â”œâ”€â”€ linear.cpp          # Linear layer implementation
â”‚   â”‚   â”œâ”€â”€ conv2d.cpp          # Convolution layer implementation
â”‚   â”‚   â”œâ”€â”€ activation.cpp      # Activation functions implementation
â”‚   â”‚   â””â”€â”€ loss.cpp            # Loss functions implementation
â”‚   â””â”€â”€ ops/
â”‚       â”œâ”€â”€ basic_ops.cpp       # Basic operations implementation
â”‚       â”œâ”€â”€ matrix_ops.cpp      # Matrix operations implementation
â”‚       â””â”€â”€ reduction_ops.cpp   # Reduction operations implementation
â”‚
â”œâ”€â”€ python/                      # Python bindings
â”‚   â”œâ”€â”€ CMakeLists.txt          # Python binding CMake file
â”‚   â”œâ”€â”€ setup.py                # Python package setup
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ binding.cpp         # Main pybind11 bindings
â”‚       â””â”€â”€ torch_wrapper.cpp   # PyTorch compatibility layer
â”‚
â”œâ”€â”€ examples/                    # Example usage
â”‚   â”œâ”€â”€ basic_operations/
â”‚   â”‚   â””â”€â”€ tensor_ops.cpp
â”‚   â””â”€â”€ neural_network/
â”‚       â””â”€â”€ simple_nn.cpp
â”‚
â”œâ”€â”€ tests/                       # Unit tests
â”‚   â”œâ”€â”€ CMakeLists.txt          # Test CMake configuration
â”‚   â”œâ”€â”€ test_tensor.cpp         # Tensor tests
â”‚   â”œâ”€â”€ test_autograd.cpp       # Autograd tests
â”‚   â”œâ”€â”€ test_nn/               # Neural network tests
â”‚   â”‚   â”œâ”€â”€ test_linear.cpp
â”‚   â”‚   â””â”€â”€ test_conv2d.cpp
â”‚   â””â”€â”€ test_ops/              # Operation tests
â”‚       â”œâ”€â”€ test_basic_ops.cpp
â”‚       â””â”€â”€ test_matrix_ops.cpp
â”‚
â””â”€â”€ docs/                       # Documentation
    â”œâ”€â”€ api/                    # API documentation
    â”œâ”€â”€ examples/              # Usage examples
    â””â”€â”€ design/               # Design documents